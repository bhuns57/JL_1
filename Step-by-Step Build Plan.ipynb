{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73be6e74-b228-46db-b2b2-935aa19f0b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tkinter as tk\n",
    "import pickle\n",
    "import os\n",
    "from datetime import datetime\n",
    "## whie testing use:\n",
    "filename = \"xl_dtv_all_snm.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda1c72-54b4-4d8b-9e39-3193f4096ad7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# ***Step-by-Step Build Plan***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6d0ef09-2e55-4b09-ad77-a6a8547e0bee",
   "metadata": {},
   "source": [
    "## ***Request to copilot to plan a system for xl csv file to \"wrk_array\"***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cfad843-3085-4a49-89d1-141977ba46c7",
   "metadata": {},
   "source": [
    "# üß† Modular Data Classification Workflow\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 1: Load CSV and Build <code>array_dict</code></summary>\n",
    "\n",
    "- Read `.pkl` files containing raw `dat_col` arrays\n",
    "- Assign each column a serial number (`snr`) for name editability\n",
    "- Construct `array_dict[snr] = np.array([...])`\n",
    "- Log metadata: source file, timestamp, column count\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 2: Embed <code>args</code> into <code>array_dict</code></summary>\n",
    "\n",
    "- Define classification parameters per `dat_col`\n",
    "- Store under `array_dict[snr]['args']`\n",
    "- Validate schema using `validate_args()` utility\n",
    "- Example args: method, threshold, flags, notes\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 3: Classify <code>dat_cols</code></summary>\n",
    "\n",
    "- Use registry-based classifier engine\n",
    "- Apply method from `args` to each `dat_col`\n",
    "- Store result in `array_dict[snr]['classification']`\n",
    "- Log classification decisions and edge cases\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 4: Filter to Create <code>results_list</code></summary>\n",
    "\n",
    "- Filter `array_dict` by classification label and method\n",
    "- Build `results_list = [snr1, snr2, ...]`\n",
    "- Optionally log rejected entries with reasons\n",
    "- Prepare for GUI-assisted selection\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 5: GUI Selection to Create <code>grp_nm</code></summary>\n",
    "\n",
    "- Use multi-Tkinter interface to display `results_list`\n",
    "- Operator selects final `dat_cols` by `snr`\n",
    "- Store selected list as `grp_nm`\n",
    "- Include preview plots or stats for judgment support\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 6: Build <code>wrk_dict</code> from <code>grp_nm</code></summary>\n",
    "\n",
    "- Structure: `wrk_dict = {'dtv': dtv_col, 'grp_nm': [dat_col1, dat_col2, ...]}`\n",
    "- Include metadata: classification method, timestamp, operator ID\n",
    "- Use for downstream analysis or diagnostics\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 7: Persist <code>grp_nm</code> for Future Use</summary>\n",
    "\n",
    "- Save `grp_nm` under a unique name with attributes\n",
    "- Store in `saved_groups['group_name'] = {...}`\n",
    "- Use Pickle for long-term storage\n",
    "- Export slices to Excel for operator edits\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10220354-ee3e-436d-a29f-50763452b747",
   "metadata": {},
   "source": [
    "https://copilot.microsoft.com/shares/pages/e9KLtXUon7NWYWbVJc18U\n",
    "\n",
    "Note: I refer to \"dat_cols\"  by serial number \"snr\" to maintain easy name editability.\n",
    "My workflow's main intermediate step is to generate a temporary \"wrk_dict\" that will be used in later specific processes.\n",
    "1. First, I read\" .csv files\" and built the \"array_dict\" with 1000s of \"dat_cols\" [this is now working reliably]\n",
    "2. The \"array_dict\" also has the data for the \"args\"  required while classifying the  \"dat_cols\"[This is evolving]\n",
    "3. Then I will classify the \"dat_col\" using the \"args\" stored in the \"array_dict\".\n",
    "4. Then I will use the \"dat_col\"  \"classification\" to filter the \"dat_col\" list of  the \"array_dict\" by attribute and method to optimally reduce the size of the \"dat_col\" list and call it the \"results_list\" \n",
    "5. Then I will use multi-tkinter on the \"results_list\" to use operator judgment and create the \"final list\" of the required \"dat_col\"s  identified by their  \"snr\". That list is called \"grp_nm\". [basics demonstrated]\n",
    "6. \"grp_nm\" is then used to build a \"wrk_dict\" containing \"dtv\" col and the \"grp_nm\" list of \"dat_col\" s\n",
    "7. Then the grp_nm is stored under a unique name with helpful attributes to be used in future \"wrk_dict\" creations.\n",
    "Does this seem like a plausible workflow? What is the best approach to start building this on a step-by-step basis?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60714cab-fa5e-4e77-a9c1-097829c63379",
   "metadata": {},
   "source": [
    "### ***ANSWER***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e3337ed-7ab6-4e92-8209-da81d67254ea",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## ANSWER\n",
    "Bill, this is not only plausible‚Äîit‚Äôs a well-structured, modular workflow with clear separation of concerns and a strong foundation for reproducibility and operator-guided refinement. Your use of serial numbers (\"snr\") for `dat_cols` is a smart abstraction for name editability, and the staged evolution from raw ingestion to GUI-assisted selection and persistent storage is textbook systems thinking.\n",
    "\n",
    "Let‚Äôs break this into a **step-by-step build plan**, with scaffolding suggestions and diagnostic hooks to keep things transparent and extensible:\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Step-by-Step Build Plan\n",
    "\n",
    "### **Step 1: CSV Ingestion ‚Üí `array_dict` Construction**\n",
    "- ‚úÖ Already working reliably.\n",
    "- **Suggestion**: Add metadata logging per file (e.g., source, timestamp, column count) to support traceability.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 2: Embed `args` into `array_dict`**\n",
    "- Store classification parameters (`args`) alongside each `dat_col` entry.\n",
    "- Use a nested structure like:\n",
    "  ```python\n",
    "  array_dict[snr] = {\n",
    "      'data': np.array(...),\n",
    "      'args': {'method': 'range_check', 'threshold': 0.8, ...}\n",
    "  }\n",
    "  ```\n",
    "- **Diagnostic Tip**: Validate `args` schema with a utility like `validate_args(array_dict)` to catch malformed entries early.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 3: Classification of `dat_cols`**\n",
    "- Build a classifier function that reads `args` and returns a label or score.\n",
    "  ```python\n",
    "  def classify_dat_col(data, args):\n",
    "      # Apply method from args\n",
    "      return classification_result\n",
    "  ```\n",
    "- Store results in `array_dict[snr]['classification']`.\n",
    "\n",
    "- **Modular Suggestion**: Use a registry pattern for classification methods to allow easy extension:\n",
    "  ```python\n",
    "  classifiers = {\n",
    "      'range_check': range_check_fn,\n",
    "      'pattern_match': pattern_match_fn,\n",
    "      ...\n",
    "  }\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 4: Filter ‚Üí `results_list`**\n",
    "- Filter `array_dict` based on classification attributes.\n",
    "  ```python\n",
    "  results_list = [\n",
    "      snr for snr, entry in array_dict.items()\n",
    "      if entry['classification'] in desired_classes\n",
    "  ]\n",
    "  ```\n",
    "- **Enhancement**: Add method-based filtering (e.g., only those classified via `range_check`).\n",
    "\n",
    "- **Diagnostic Overlay**: Log rejected `snr`s with reasons to support operator review.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 5: GUI Selection ‚Üí `grp_nm`**\n",
    "- Use multi-Tkinter to display `results_list` with classification summaries.\n",
    "- Allow operator to select final `snr`s for inclusion.\n",
    "- Store selected list as `grp_nm`.\n",
    "\n",
    "- **Tip**: Include preview plots or stats per `dat_col` to aid judgment.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 6: Build `wrk_dict`**\n",
    "- Structure:\n",
    "  ```python\n",
    "  wrk_dict = {\n",
    "      'dtv': dtv_col,\n",
    "      'grp_nm': [array_dict[snr]['data'] for snr in grp_nm]\n",
    "  }\n",
    "  ```\n",
    "- **Optional**: Include metadata like classification method, timestamp, operator ID.\n",
    "\n",
    "---\n",
    "\n",
    "### **Step 7: Persist `grp_nm` for Future Use**\n",
    "- Save `grp_nm` under a unique name with attributes:\n",
    "  ```python\n",
    "  saved_groups['group_name'] = {\n",
    "      'snr_list': grp_nm,\n",
    "      'attributes': {'method': 'range_check', 'created': timestamp, ...}\n",
    "  }\n",
    "  ```\n",
    "- **Suggestion**: Use JSON or pickle for persistence, with versioning.\n",
    "\n",
    "---\n",
    "\n",
    "## üõ†Ô∏è Diagnostic & Extension Hooks\n",
    "\n",
    "- **Logging**: Add per-step logs with timestamps and summary stats.\n",
    "- **Validation**: Build schema checkers for `args`, `classification`, and `wrk_dict`.\n",
    "- **Batch Utilities**: Integrate bit masking or packing for efficient storage of classification flags.\n",
    "- **GUI Enhancements**: Add search/filter in Tkinter for large `results_list`.\n",
    "\n",
    "---\n",
    "\n",
    "## üöÄ Next Steps\n",
    "\n",
    "If you're ready to start building, I‚Äôd suggest scaffolding **Step 2 and Step 3** next‚Äîembedding `args` and building the classifier registry. I can help you write those utilities modularly and test them with mock `dat_cols`.\n",
    "\n",
    "Would you like to start with a classification registry scaffold and a validation utility for `args`?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03123831-a5d8-4c82-8296-312f07599c2a",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Resolving Special Issues that impact the plan"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb594f57-cd31-4665-848f-a0a49da9e730",
   "metadata": {},
   "source": [
    "## ***csv files don't work with dictionaries***"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e7f48b5-2796-4366-b344-33239265be4e",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## You‚Äôre absolutely right, Bill‚Äîonce your `array_dict` evolves to include flags, attributes, and classification metadata, flat `.csv` storage becomes too brittle and lossy. You need a format that preserves **hierarchical structure**, **data types**, and **extensibility**. Here‚Äôs a breakdown of the best options for storing this richer data:\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Recommended Storage Formats for `array_dict`\n",
    "\n",
    "### **1. Pickle (`.pkl`) ‚Äî Python-native serialization**\n",
    "- ‚úÖ **Pros**:\n",
    "  - Preserves full Python objects (dicts, arrays, classes)\n",
    "  - Fast read/write\n",
    "  - Ideal for intermediate storage during development\n",
    "- ‚ùå **Cons**:\n",
    "  - Not human-readable\n",
    "  - Not portable across languages\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "\n",
    "# Save\n",
    "with open('array_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(array_dict, f)\n",
    "\n",
    "# Load\n",
    "with open('array_dict.pkl', 'rb') as f:\n",
    "    array_dict = pickle.load(f)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. JSON (`.json`) ‚Äî Human-readable, semi-structured**\n",
    "- ‚úÖ **Pros**:\n",
    "  - Easy to inspect and edit manually\n",
    "  - Portable across platforms\n",
    "- ‚ùå **Cons**:\n",
    "  - Doesn‚Äôt support NumPy arrays or complex types natively\n",
    "  - Requires conversion (e.g., `.tolist()` for arrays)\n",
    "\n",
    "```python\n",
    "import json\n",
    "\n",
    "# Convert arrays to lists\n",
    "json_ready_dict = {\n",
    "    snr: {\n",
    "        'data': array_dict[snr]['data'].tolist(),\n",
    "        'args': array_dict[snr]['args'],\n",
    "        'classification': array_dict[snr]['classification']\n",
    "    }\n",
    "    for snr in array_dict\n",
    "}\n",
    "\n",
    "# Save\n",
    "with open('array_dict.json', 'w') as f:\n",
    "    json.dump(json_ready_dict, f, indent=2)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. HDF5 (`.h5`) ‚Äî Structured, scalable, great for arrays**\n",
    "- ‚úÖ **Pros**:\n",
    "  - Handles large datasets and hierarchical metadata\n",
    "  - Efficient for NumPy arrays and numerical data\n",
    "- ‚ùå **Cons**:\n",
    "  - Requires `h5py` or `pandas` interface\n",
    "  - Less intuitive than JSON or Pickle\n",
    "\n",
    "```python\n",
    "import h5py\n",
    "\n",
    "with h5py.File('array_dict.h5', 'w') as f:\n",
    "    for snr, entry in array_dict.items():\n",
    "        grp = f.create_group(str(snr))\n",
    "        grp.create_dataset('data', data=entry['data'])\n",
    "        for k, v in entry['args'].items():\n",
    "            grp.attrs[k] = v\n",
    "        grp.attrs['classification'] = entry['classification']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **4. SQLite ‚Äî If you want queryable persistence**\n",
    "- ‚úÖ **Pros**:\n",
    "  - Structured, searchable, supports indexing\n",
    "  - Good for long-term storage and retrieval\n",
    "- ‚ùå **Cons**:\n",
    "  - Requires schema design\n",
    "  - Not ideal for raw arrays unless stored as blobs\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Suggested Strategy for You\n",
    "\n",
    "Since you‚Äôre in **active development**, I recommend:\n",
    "\n",
    "- **Use Pickle** for fast, full-fidelity saves during iteration\n",
    "- **Export to JSON** for human-readable snapshots or debugging\n",
    "- **Consider HDF5** if you scale to large datasets or want structured archival\n",
    "\n",
    "Later, if you want to build a persistent database of `wrk_dict`s or `grp_nm` sets, we can scaffold a lightweight SQLite schema with metadata indexing.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like me to scaffold a Pickle + JSON dual-save utility that automatically converts arrays and preserves classification metadata? That could be your go-to tool for saving `array_dict` at any stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a0e36e6-306e-4e83-ae73-40582f41c671",
   "metadata": {},
   "source": [
    "## Hybrid Workflow: Pickle + Excel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea689dbc-dd60-48ba-a23d-c47fc511e329",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "## Exactly, Bill‚Äîthat‚Äôs a smart hybrid approach. You can absolutely use **Pickle for long-term, full-fidelity storage** of your `array_dict`, and selectively **export individual columns or metadata slices to Excel** for manual review or operator edits. Then, re-import those edits and patch them back into the dictionary. Here's how that workflow could look:\n",
    "\n",
    "---\n",
    "\n",
    "## üîÅ Hybrid Workflow: Pickle + Excel\n",
    "\n",
    "### **1. Store Full `array_dict` in Pickle**\n",
    "- Preserves all nested structures, NumPy arrays, flags, and classification metadata.\n",
    "- Ideal for versioned snapshots and reproducible workflows.\n",
    "\n",
    "```python\n",
    "with open('array_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(array_dict, f)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Export a Column Slice to Excel**\n",
    "- Extract just the part you want to edit‚Äîe.g., classification labels or `args`.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Example: Export classification labels\n",
    "df = pd.DataFrame({\n",
    "    'snr': list(array_dict.keys()),\n",
    "    'classification': [entry.get('classification', '') for entry in array_dict.values()]\n",
    "})\n",
    "\n",
    "df.to_excel('classification_review.xlsx', index=False)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Edit in Excel**\n",
    "- Operator can review, adjust labels, add comments, or flag entries.\n",
    "- You can even include columns like `reason_for_change` or `reviewer_id`.\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Re-import and Patch Back**\n",
    "- Read the edited Excel file and update `array_dict` accordingly.\n",
    "\n",
    "```python\n",
    "edited_df = pd.read_excel('classification_review.xlsx')\n",
    "\n",
    "for _, row in edited_df.iterrows():\n",
    "    snr = row['snr']\n",
    "    new_class = row['classification']\n",
    "    if snr in array_dict:\n",
    "        array_dict[snr]['classification'] = new_class\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Re-save to Pickle**\n",
    "- Once patched, you can re-save the updated dictionary.\n",
    "\n",
    "```python\n",
    "with open('array_dict.pkl', 'wb') as f:\n",
    "    pickle.dump(array_dict, f)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Bonus Tips\n",
    "\n",
    "- **Version your Excel exports**: e.g., `classification_review_v3.xlsx`\n",
    "- **Log changes**: Keep a change log dictionary or CSV for traceability.\n",
    "- **Validate edits**: Run a schema check after import to catch typos or invalid labels.\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a utility function that automates this export‚Äìedit‚Äìpatch cycle for a given key like `'classification'` or `'args'`? I can scaffold that next."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079d2a5e-7fd1-4cfb-9ec3-e854e723fbb8",
   "metadata": {},
   "source": [
    "## Backup and git when using pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0a8e9b6-702f-42a5-8fa5-5da5ea47b4e2",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Git can technically track `.pkl` files, but it‚Äôs not ideal for versioning or diffing them. Here‚Äôs the breakdown:\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Git + Pickle: What Works, What Doesn‚Äôt\n",
    "\n",
    "### ‚úÖ **Git can store `.pkl` files**\n",
    "- You can commit, push, and pull them like any other binary file.\n",
    "- Useful for syncing across machines or collaborators.\n",
    "\n",
    "### ‚ùå **Git can‚Äôt diff `.pkl` files**\n",
    "- Pickle files are binary blobs‚ÄîGit can‚Äôt show what changed inside.\n",
    "- No line-by-line history, no merge conflict resolution.\n",
    "\n",
    "---\n",
    "\n",
    "## üîê Best Practice: Use Git + External Backups\n",
    "\n",
    "### üîÑ **Recommended Strategy**\n",
    "| Purpose               | Format         | Tool            |\n",
    "|----------------------|----------------|-----------------|\n",
    "| Code & logic         | `.py`, `.md`   | Git             |\n",
    "| Data snapshots       | `.pkl`         | Git (basic)     |\n",
    "| Editable metadata    | `.json`, `.xlsx` | Git + Excel     |\n",
    "| Long-term backup     | `.pkl`, `.h5`  | External (e.g., cloud, USB) |\n",
    "\n",
    "### üß© Add a `data/` folder to your repo\n",
    "- Store `.pkl` files there\n",
    "- Use `.gitignore` if files get too large or sensitive\n",
    "\n",
    "```bash\n",
    "# Example .gitignore entry\n",
    "data/*.pkl\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üõ°Ô∏è Backup Suggestions\n",
    "- **Cloud sync**: OneDrive, Dropbox, Google Drive\n",
    "- **Versioned exports**: `array_dict_v1.pkl`, `array_dict_v2.pkl`\n",
    "- **Manual snapshots**: Save key milestones with timestamps\n",
    "\n",
    "---\n",
    "\n",
    "If you want, I can help scaffold a versioning utility that saves your `array_dict` with auto-incremented filenames and logs the changes. That way, you get traceability even outside Git."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be4eb42a-eb78-4d2a-8c8f-f56ab25637c9",
   "metadata": {},
   "source": [
    "## .h5 ‚Äî HDF5 File Format"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fde044f-4488-47f9-9919-1d7e77be2cf7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Great question, Bill‚Äîthese two formats serve very different purposes, and both are quite handy in your workflow depending on what you're building or documenting.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ `.h5` ‚Äî HDF5 File Format\n",
    "\n",
    "**HDF5** stands for **Hierarchical Data Format version 5**. It‚Äôs a powerful binary format designed for storing large, complex datasets‚Äîespecially arrays and structured metadata.\n",
    "\n",
    "### üîß Key Features:\n",
    "- Stores **NumPy arrays**, matrices, and nested groups\n",
    "- Supports **attributes** (like flags, labels, timestamps)\n",
    "- Efficient for **read/write** operations on large datasets\n",
    "- Ideal for scientific computing, machine learning, and modular health analytics\n",
    "\n",
    "### üß™ Example Use Case:\n",
    "You could store your entire `array_dict` in `.h5`, with each `snr` as a group containing:\n",
    "- `'data'`: the NumPy array\n",
    "- `'args'`: stored as attributes\n",
    "- `'classification'`: also as an attribute or dataset\n",
    "\n",
    "### üõ†Ô∏è Tools:\n",
    "- Use Python‚Äôs `h5py` or `pandas` to read/write\n",
    "- Cross-language support (e.g., MATLAB, R, C++)\n",
    "\n",
    "---\n",
    "\n",
    "## üìò `.md` ‚Äî Markdown File\n",
    "\n",
    "**Markdown** is a lightweight markup language used for writing formatted text using plain syntax. Files with `.md` extension are typically used for documentation.\n",
    "\n",
    "### ‚úçÔ∏è Key Features:\n",
    "- Easy to write and read\n",
    "- Converts to HTML for web display\n",
    "- Supports headings, lists, code blocks, links, tables\n",
    "\n",
    "### üìÑ Example Use Case:\n",
    "You could use `.md` files to document:\n",
    "- Your classification methods\n",
    "- Diagnostic overlays\n",
    "- GUI workflows\n",
    "- Version history of `wrk_dict` builds\n",
    "\n",
    "### üß† Bonus:\n",
    "GitHub and many IDEs render `.md` beautifully, so it‚Äôs perfect for collaborative documentation.\n",
    "\n",
    "---\n",
    "\n",
    "## üß© Summary Table\n",
    "\n",
    "| Format | Purpose | Best For |\n",
    "|--------|---------|----------|\n",
    "| `.h5`  | Binary data storage | Arrays, metadata, scalable datasets |\n",
    "| `.md`  | Documentation | Notes, instructions, version logs |\n",
    "\n",
    "---\n",
    "\n",
    "If you‚Äôd like, I can help scaffold an `.h5` save/load utility for your `array_dict`, or a Markdown template to document your classification modules. Just say the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d0d988-8fe1-41ec-b36a-3f760652cce6",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Why Pickle Is Perfect for Your Use Case"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10575b3d-5447-42d8-931f-c61e1a4d9192",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Here‚Äôs why your instinct is solid:\n",
    "\n",
    "---\n",
    "\n",
    "## ‚úÖ Why Pickle Is Perfect for Your Use Case\n",
    "\n",
    "- **Fast and frictionless**: No schema design, no conversion overhead‚Äîjust save and load.\n",
    "- **Preserves structure**: Your `array_dict` with nested keys, NumPy arrays, and metadata stays intact.\n",
    "- **Lightweight footprint**: Even with 5,000√ó1,000 entries, Pickle handles it effortlessly unless you're storing massive arrays per entry.\n",
    "- **Ideal for iterative workflows**: You‚Äôre refining, classifying, and patching‚ÄîPickle lets you checkpoint without ceremony.\n",
    "\n",
    "---\n",
    "\n",
    "## üß† When HDF5 Might Be Worth It (but not now)\n",
    "\n",
    "- You‚Äôre storing **gigabytes** of raw array data per `dat_col`\n",
    "- You need **partial reads/writes** (e.g., load one `snr` without reading the whole file)\n",
    "- You‚Äôre integrating with **non-Python tools** (e.g., MATLAB, R)\n",
    "\n",
    "But for your modular health system, where clarity, traceability, and operator-guided refinement matter more than raw throughput‚ÄîPickle wins.\n",
    "\n",
    "---\n",
    "\n",
    "## üîß Suggested Practice\n",
    "\n",
    "- Use Pickle for your working `array_dict` snapshots\n",
    "- Export slices (e.g., classification labels) to Excel for operator edits\n",
    "- Keep a versioned folder like:\n",
    "  ```\n",
    "  /data_snapshots/\n",
    "    array_dict_v1.pkl\n",
    "    array_dict_v2.pkl\n",
    "    classification_review_v1.xlsx\n",
    "  ```\n",
    "\n",
    "---\n",
    "\n",
    "If you'd like, I can scaffold a utility that:\n",
    "- Saves your `array_dict` to Pickle\n",
    "- Optionally exports a metadata slice to Excel\n",
    "- Logs the save with a timestamp and version tag\n",
    "\n",
    "Would that help streamline your workflow?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6e874e-2bc6-46cd-8c30-2905dde16609",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Scaffold a utility that:\n",
    "1. Saves your array_dict to Pickle\n",
    "2. Optionally exports a metadata slice to Excel\n",
    "3. Logs the save with a timestamp and version tag"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6678b503-612d-4652-99f4-fa96320fbe6d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Perfect, Bill. Here's a modular utility scaffold that handles:\n",
    "\n",
    "- ‚úÖ Saving your `array_dict` to Pickle\n",
    "- ‚úÖ Exporting a metadata slice (like `'classification'`) to Excel\n",
    "- ‚úÖ Logging the save with a timestamp and version tag\n",
    "\n",
    "## üß∞ `array_dict_saver.py`\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "def save_array_dict(array_dict, save_dir='data_snapshots', tag='v1', export_key='classification'):\n",
    "    \"\"\"\n",
    "    Saves array_dict to a Pickle file and exports a metadata slice to Excel.\n",
    "    \n",
    "    Parameters:\n",
    "        array_dict (dict): Your main data dictionary\n",
    "        save_dir (str): Folder to store snapshots\n",
    "        tag (str): Version tag (e.g., 'v1', 'v2')\n",
    "        export_key (str): Key to export to Excel (e.g., 'classification')\n",
    "    \"\"\"\n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save Pickle\n",
    "    pkl_path = os.path.join(save_dir, f'array_dict_{tag}_{timestamp}.pkl')\n",
    "    with open(pkl_path, 'wb') as f:\n",
    "        pickle.dump(array_dict, f)\n",
    "    \n",
    "    # Export metadata slice to Excel\n",
    "    excel_data = {\n",
    "        'snr': [],\n",
    "        export_key: []\n",
    "    }\n",
    "    for snr, entry in array_dict.items():\n",
    "        excel_data['snr'].append(snr)\n",
    "        excel_data[export_key].append(entry.get(export_key, ''))\n",
    "\n",
    "    df = pd.DataFrame(excel_data)\n",
    "    xlsx_path = os.path.join(save_dir, f'{export_key}_review_{tag}_{timestamp}.xlsx')\n",
    "    df.to_excel(xlsx_path, index=False)\n",
    "\n",
    "    print(f\"‚úÖ Saved Pickle to: {pkl_path}\")\n",
    "    print(f\"üì§ Exported {export_key} to Excel: {xlsx_path}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example Usage\n",
    "\n",
    "```python\n",
    "save_array_dict(array_dict, tag='v3', export_key='classification')\n",
    "```\n",
    "\n",
    "This will create:\n",
    "- `data_snapshots/array_dict_v3_20250905_1351.pkl`\n",
    "- `data_snapshots/classification_review_v3_20250905_1351.xlsx`\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Optional Enhancements\n",
    "- Add a `log.txt` file to track saves and changes\n",
    "- Include other keys like `'args'` or `'flags'` in the Excel export\n",
    "- Add a `load_array_dict()` function to restore snapshots easily\n",
    "\n",
    "---\n",
    "\n",
    "Want me to scaffold the reverse utility next‚Äîwhere you patch edited Excel values back into `array_dict`? That would complete the loop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6f9a7d-3fa1-45e5-89f7-2f7a66105d6f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "### Companion utility that\n",
    "1. patches edited Excel values back into your array_dict,\n",
    "2. completing the loop\n",
    "3. with full traceability and modular control."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d603ff1b-cdf5-48f0-9b9a-6393ca36747c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Companion utility that **patches edited Excel values back into your `array_dict`**, completing the loop with full traceability and modular control.\n",
    "---\n",
    "\n",
    "## üîß `patch_array_dict_from_excel.py`\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "def patch_array_dict_from_excel(array_dict, excel_path, key_to_patch='classification'):\n",
    "    \"\"\"\n",
    "    Reads an Excel file and updates array_dict entries based on edited values.\n",
    "    \n",
    "    Parameters:\n",
    "        array_dict (dict): Your working dictionary\n",
    "        excel_path (str): Path to the edited Excel file\n",
    "        key_to_patch (str): Which key to update (e.g., 'classification', 'args')\n",
    "    \n",
    "    Returns:\n",
    "        int: Number of entries successfully patched\n",
    "    \"\"\"\n",
    "    edited_df = pd.read_excel(excel_path)\n",
    "    patched_count = 0\n",
    "\n",
    "    for _, row in edited_df.iterrows():\n",
    "        snr = row['snr']\n",
    "        new_value = row[key_to_patch]\n",
    "        if snr in array_dict:\n",
    "            array_dict[snr][key_to_patch] = new_value\n",
    "            patched_count += 1\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è Warning: snr '{snr}' not found in array_dict\")\n",
    "\n",
    "    print(f\"‚úÖ Patched {patched_count} entries from: {excel_path}\")\n",
    "    return patched_count\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß™ Example Usage\n",
    "\n",
    "```python\n",
    "patch_array_dict_from_excel(array_dict, 'data_snapshots/classification_review_v3_20250905_1351.xlsx')\n",
    "```\n",
    "\n",
    "This will:\n",
    "- Read the edited Excel file\n",
    "- Update each `array_dict[snr]['classification']` with the new value\n",
    "- Warn you if any `snr` is missing (e.g., typo or deletion)\n",
    "\n",
    "---\n",
    "\n",
    "## üß† Optional Enhancements\n",
    "\n",
    "- Add a **change log** dictionary to track old vs. new values\n",
    "- Support patching nested keys (e.g., inside `'args'`)\n",
    "- Validate new values against allowed types or labels\n",
    "\n",
    "---\n",
    "\n",
    "If you‚Äôd like, I can extend this to support patching multiple keys at once (e.g., `'classification'`, `'flags'`, `'notes'`) or even auto-save the patched dictionary to Pickle afterward. Just say the word and I‚Äôll scaffold it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf19d63-4957-4a44-922e-9808e0e519df",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Example: Collapsible Markdown Outline\n",
    "https://copilot.microsoft.com/shares/pages/MbLzW1xQx8RkYvdpC2SYG"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94e5b601-f14d-4b72-b947-c50147c66ff5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "</details>\n",
    "<summary>üìÅYou can collapse sections of a Markdown outline using **HTML tags**‚Äîspecifically `<details>` and `<summary>`. While standard Markdown doesn‚Äôt support collapsible sections natively, GitHub-flavored Markdown and many Markdown renderers (like VS Code, Jupyter, and some static site generators) do support this hybrid syntax.\n",
    "\n",
    "---\n",
    "\n",
    "## üì¶ Example: Collapsible Markdown Outline\n",
    "\n",
    "```markdown\n",
    "<details>\n",
    "<summary>üìÅ Step 1: Load CSV and Build array_dict</summary>\n",
    "\n",
    "- Read `.csv` files\n",
    "- Parse into `array_dict` using `snr` keys\n",
    "- Validate structure and log metadata\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 2: Embed args into array_dict</summary>\n",
    "\n",
    "- Define classification parameters\n",
    "- Store under `array_dict[snr]['args']`\n",
    "- Validate schema with `validate_args()`\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 3: Classify dat_cols</summary>\n",
    "\n",
    "- Use registry-based classifier\n",
    "- Store result in `array_dict[snr]['classification']`\n",
    "- Log decisions and edge cases\n",
    "\n",
    "</details>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## üß† How It Works\n",
    "\n",
    "- `<details>` wraps the collapsible section\n",
    "- `<summary>` is the clickable header\n",
    "- Inside the block, you can use **any Markdown**: lists, code, headers, etc.\n",
    "- Most renderers show a ‚ñ∂ icon that expands to ‚ñº when clicked\n",
    "\n",
    "---\n",
    "\n",
    "## ‚ö†Ô∏è Caveats\n",
    "\n",
    "- Not all Markdown engines support this (e.g., some basic editors or viewers)\n",
    "- Styling is browser-dependent unless you add custom CSS\n",
    "\n",
    "---\n",
    "\n",
    "Would you like a full collapsible outline for your 7-step workflow scaffolded in Markdown? I can generate it with summaries and nested bullet points.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b756a662-fde7-48c8-81f9-a4ae8fa4f833",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "source": [
    "Here‚Äôs your collapsible Markdown outline for the full **Modular Data Classification Workflow**, designed to be readable, scannable, and expandable in GitHub, VS Code, Jupyter, and other Markdown-friendly environments:\n",
    "\n",
    "---\n",
    "\n",
    "```markdown\n",
    "# üß† Modular Data Classification Workflow\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 1: Load CSV and Build <code>array_dict</code></summary>\n",
    "\n",
    "- Read `.csv` files containing raw `dat_col` arrays\n",
    "- Assign each column a serial number (`snr`) for name editability\n",
    "- Construct `array_dict[snr] = np.array([...])`\n",
    "- Log metadata: source file, timestamp, column count\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 2: Embed <code>args</code> into <code>array_dict</code></summary>\n",
    "\n",
    "- Define classification parameters per `dat_col`\n",
    "- Store under `array_dict[snr]['args']`\n",
    "- Validate schema using `validate_args()` utility\n",
    "- Example args: method, threshold, flags, notes\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 3: Classify <code>dat_cols</code></summary>\n",
    "\n",
    "- Use registry-based classifier engine\n",
    "- Apply method from `args` to each `dat_col`\n",
    "- Store result in `array_dict[snr]['classification']`\n",
    "- Log classification decisions and edge cases\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 4: Filter to Create <code>results_list</code></summary>\n",
    "\n",
    "- Filter `array_dict` by classification label and method\n",
    "- Build `results_list = [snr1, snr2, ...]`\n",
    "- Optionally log rejected entries with reasons\n",
    "- Prepare for GUI-assisted selection\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 5: GUI Selection to Create <code>grp_nm</code></summary>\n",
    "\n",
    "- Use multi-Tkinter interface to display `results_list`\n",
    "- Operator selects final `dat_cols` by `snr`\n",
    "- Store selected list as `grp_nm`\n",
    "- Include preview plots or stats for judgment support\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 6: Build <code>wrk_dict</code> from <code>grp_nm</code></summary>\n",
    "\n",
    "- Structure: `wrk_dict = {'dtv': dtv_col, 'grp_nm': [dat_col1, dat_col2, ...]}`\n",
    "- Include metadata: classification method, timestamp, operator ID\n",
    "- Use for downstream analysis or diagnostics\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 7: Persist <code>grp_nm</code> for Future Use</summary>\n",
    "\n",
    "- Save `grp_nm` under a unique name with attributes\n",
    "- Store in `saved_groups['group_name'] = {...}`\n",
    "- Use Pickle for long-term storage\n",
    "- Export slices to Excel for operator edits\n",
    "\n",
    "</details>\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "This structure lets you collapse and expand each step as needed, keeping your documentation clean and navigable. If you'd like, I can also generate a Markdown version with embedded code snippets or links to your utilities as you build them. Just say the word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26142a15-1f1a-4019-830d-4f3f93d59aa1",
   "metadata": {},
   "source": [
    "## Modular Data Classification Workflow\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 1: Load CSV and Build <code>array_dict</code></summary>\n",
    "\n",
    "- Read `.csv` files containing raw `dat_col` arrays\n",
    "- Assign each column a serial number (`snr`) for name editability\n",
    "- Construct `array_dict[snr] = np.array([...])`\n",
    "- Log metadata: source file, timestamp, column count\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 2: Embed <code>args</code> into <code>array_dict</code></summary>\n",
    "\n",
    "- Define classification parameters per `dat_col`\n",
    "- Store under `array_dict[snr]['args']`\n",
    "- Validate schema using `validate_args()` utility\n",
    "- Example args: method, threshold, flags, notes\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 3: Classify <code>dat_cols</code></summary>\n",
    "\n",
    "- Use registry-based classifier engine\n",
    "- Apply method from `args` to each `dat_col`\n",
    "- Store result in `array_dict[snr]['classification']`\n",
    "- Log classification decisions and edge cases\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 4: Filter to Create <code>results_list</code></summary>\n",
    "\n",
    "- Filter `array_dict` by classification label and method\n",
    "- Build `results_list = [snr1, snr2, ...]`\n",
    "- Optionally log rejected entries with reasons\n",
    "- Prepare for GUI-assisted selection\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 5: GUI Selection to Create <code>grp_nm</code></summary>\n",
    "\n",
    "- Use multi-Tkinter interface to display `results_list`\n",
    "- Operator selects final `dat_cols` by `snr`\n",
    "- Store selected list as `grp_nm`\n",
    "- Include preview plots or stats for judgment support\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 6: Build <code>wrk_dict</code> from <code>grp_nm</code></summary>\n",
    "\n",
    "- Structure: `wrk_dict = {'dtv': dtv_col, 'grp_nm': [dat_col1, dat_col2, ...]}`\n",
    "- Include metadata: classification method, timestamp, operator ID\n",
    "- Use for downstream analysis or diagnostics\n",
    "\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "<summary>üìÅ Step 7: Persist <code>grp_nm</code> for Future Use</summary>\n",
    "\n",
    "- Save `grp_nm` under a unique name with attributes\n",
    "- Store in `saved_groups['group_name'] = {...}`\n",
    "- Use Pickle for long-term storage\n",
    "- Export slices to Excel for operator edits\n",
    "\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2005c798-9953-4008-97d6-337cfe7f2ac8",
   "metadata": {},
   "source": [
    "### .h5 ‚Äî HDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdad41ef-db72-44b7-97c4-d631f7df59bc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "73ec09d6-4320-4161-ab3d-fadb7c3c49c6",
   "metadata": {},
   "source": [
    "### .h5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75b5c958-647e-49d7-9db0-b4c4a308a52d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bcc54ce3-505c-4063-86ab-1e32c03573a5",
   "metadata": {},
   "source": [
    "# STEP BY STEP IMPEMENTATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "683f57b9-e7a5-4c2e-be99-4719f72e7abd",
   "metadata": {},
   "source": [
    "To kick off **Part 2 (embedding `args` into `array_dict`)** and **Part 3 (classification of `dat_cols`)**, here‚Äôs what would help me scaffold the most effective and extensible solution for you:\n",
    "\n",
    "---\n",
    "\n",
    "## üîç Key Info I Need from You\n",
    "\n",
    "### **1. Structure of `array_dict`**\n",
    "- What does each entry currently look like?\n",
    "  - Example: `array_dict[snr] = np.array([...])` or something more nested?\n",
    "- Are `snr`s integers, strings, or mixed?\n",
    "- Do you already store metadata alongside the data?\n",
    "\n",
    "---\n",
    "\n",
    "### **2. Desired `args` Format**\n",
    "- What kind of classification parameters do you want to store per `dat_col`?\n",
    "  - Examples:\n",
    "    - Thresholds (`min`, `max`)\n",
    "    - Method type (`range_check`, `pattern_match`, etc.)\n",
    "    - Flags (`is_active`, `requires_review`)\n",
    "- Should `args` be uniform across all `dat_cols`, or vary per entry?\n",
    "\n",
    "---\n",
    "\n",
    "### **3. Classification Logic**\n",
    "- What kinds of classification methods are you planning to use?\n",
    "  - Examples:\n",
    "    - Range-based filtering\n",
    "    - Pattern detection (e.g., spikes, flatlines)\n",
    "    - Statistical checks (e.g., std deviation, kurtosis)\n",
    "- Do you want the classifier to return:\n",
    "  - A label (e.g., `\"valid\"`, `\"noisy\"`)\n",
    "  - A score (e.g., `0.92`)\n",
    "  - A boolean flag?\n",
    "\n",
    "---\n",
    "\n",
    "### **4. Output Format of Classification**\n",
    "- Should the result be stored inside `array_dict[snr]`, like:\n",
    "  ```python\n",
    "  array_dict[snr] = {\n",
    "      'data': np.array([...]),\n",
    "      'args': {...},\n",
    "      'classification': 'valid'\n",
    "  }\n",
    "  ```\n",
    "- Or do you prefer a separate dictionary like `classification_dict[snr] = ...`?\n",
    "\n",
    "---\n",
    "\n",
    "### **5. Diagnostic Preferences**\n",
    "- Do you want:\n",
    "  - Logging of classification decisions?\n",
    "  - A summary report of how many passed/failed?\n",
    "  - A debug mode to visualize borderline cases?\n",
    "\n",
    "---\n",
    "\n",
    "Once I have this, I can scaffold:\n",
    "- A flexible `args` embedding utility\n",
    "- A registry-based classifier engine\n",
    "- A validation tool to catch malformed `args`\n",
    "- Optional logging and summary diagnostics\n",
    "\n",
    "Would you like to start by showing me a sample `array_dict` entry and one or two classification methods you‚Äôd like to implement first? That‚Äôll give me a solid foundation to build from."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dd2d1d1-2661-4397-b6ef-8db6dff7531a",
   "metadata": {},
   "source": [
    "### Utility functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40065323-d68c-4fd5-90d9-2570768fb753",
   "metadata": {},
   "source": [
    "###  Dictionaries and Storage"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2b378c-fc23-4f93-8584-c09d3ee0e164",
   "metadata": {},
   "source": [
    "### class structures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f201f9dd-2f56-4e82-aedd-5b46aabb55d5",
   "metadata": {},
   "source": [
    "### def functions and tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a0345bd-2fc1-4659-98f2-88516fecbde3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff39a76-28fc-401d-bcf5-5f45473c4c3e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a7df1-ffea-4f93-83af-4d1052c557d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffaba8c0-8e60-442d-a9a5-2c2eb7bfee9c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (scu)",
   "language": "python",
   "name": "scu"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
